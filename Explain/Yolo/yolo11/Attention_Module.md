# Attention Module trong YOLO ‚Äî Gi·∫£i th√≠ch chi ti·∫øt + V√≠ d·ª• minh h·ªça

## 1. Gi·ªõi thi·ªáu

M·ª•c ti√™u c·ªßa **Attention** l√† cho ph√©p m·ªói v·ªã tr√≠ (pixel/patch) trong ·∫£nh **li√™n k·∫øt** v·ªõi t·∫•t c·∫£ c√°c v·ªã tr√≠ kh√°c, ch·ª© kh√¥ng ch·ªâ nh√¨n local nh∆∞ convolution.  
Nh·ªù ƒë√≥, m√¥ h√¨nh h·ªçc ƒë∆∞·ª£c c·∫£:

- Quan h·ªá g·∫ßn (local features).
- Quan h·ªá xa (global context).

---

## 2. Tham s·ªë quan tr·ªçng

- `dim`: t·ªïng s·ªë k√™nh c·ªßa feature ƒë·∫ßu v√†o.
- `num_heads`: chia k√™nh th√†nh nhi·ªÅu *head* ƒë·ªÉ h·ªçc song song.
- `attn_ratio`: ƒë·ªãnh nghƒ©a k√≠ch th∆∞·ªõc c·ªßa Query/Key so v·ªõi Value.

üëâ Hi·ªÉu n√¥m na: **Value** ch·ª©a th√¥ng tin "n·ªôi dung", c√≤n **Query/Key** ch·ªâ ƒë√≥ng vai tr√≤ "so kh·ªõp m·ª©c ƒë·ªô li√™n quan".  
V√¨ th·∫ø th∆∞·ªùng cho Q,K nh·ªè h∆°n V ƒë·ªÉ gi·∫£m chi ph√≠ t√≠nh to√°n.

---

## 3. C√°c thu·ªôc t√≠nh ch√≠nh

- **`head_dim = dim // num_heads`**  
  ‚Üí m·ªói head x·ª≠ l√Ω m·ªôt l√°t c·∫Øt k√™nh ri√™ng bi·ªát.

- **`key_dim = int(head_dim * attn_ratio)`**  
  ‚Üí quy·∫øt ƒë·ªãnh k√≠ch th∆∞·ªõc Q, K.

- **`scale = 1 / \sqrt{key_dim}`**  
  ‚Üí n·∫øu kh√¥ng c√≥ scale, khi t√≠nh $QK^T$, gi√° tr·ªã c√≥ th·ªÉ l·ªõn ‚Üí softmax saturate ‚Üí gradient vanish.

- **`qkv`**: Conv1√ó1 t·∫°o Q, K, V c√πng l√∫c.

- **`proj`**: Conv1√ó1 h·ª£p nh·∫•t k·∫øt qu·∫£ t·ª´ nhi·ªÅu head.

- **`pe`**: Conv3√ó3 (group conv) ƒë·ªÉ b·ªï sung th√¥ng tin v·ªã tr√≠ (positional encoding).

---

## 4. Lu·ªìng x·ª≠ l√Ω (forward)

‚ö° **[Q, K, V l√† g√¨](../yolo11/EX/Q,K,V%20-%20Attention.md)**

### B∆∞·ªõc 1 ‚Äî Sinh Q, K, V

- Input: $x$ c√≥ shape $(B, C, H, W)$
- Sau `Conv1√ó1`: t·∫°o tensor g·ªôp `[Q, K, V]`.
- Split:

$$
\begin{aligned}
Q &: (B, \text{num}_{\text{heads}}, \text{key}_{\text{dim}}, N) \\
K &: (B, \text{num}_{\text{heads}}, \text{key}_{\text{dim}}, N) \\
V &: (B, \text{num}_{\text{heads}}, \text{head}_{\text{dim}}, N)
\end{aligned}
$$



v·ªõi $N = H \times W$ (t·ªïng s·ªë pixel).

---

### B∆∞·ªõc 2 ‚Äî Attention Score

T√≠nh ƒë·ªô t∆∞∆°ng t·ª± (similarity) gi·ªØa Q v√† K:

$$
S = Q^T K \cdot \text{scale}
$$

- $S$ c√≥ shape $(B, \text{num}_{\text{heads}}, N, N)$.
- M·ªói ph·∫ßn t·ª≠ $s_{ij}$ = m·ª©c ƒë·ªô li√™n quan c·ªßa v·ªã tr√≠ $i$ ƒë·∫øn v·ªã tr√≠ $j$.

Chu·∫©n h√≥a softmax theo t·ª´ng h√†ng:

$$
\text{attn}_{ij} = \frac{\exp(s_{ij})}{\sum_{j} \exp(s_{ij})}
$$

---

### B∆∞·ªõc 3 ‚Äî K·∫øt h·ª£p v·ªõi V

T·∫°o output b·∫±ng c√°ch nh√¢n attention v·ªõi V:

$$
O = \text{attn} \cdot V^T
$$

---

### B∆∞·ªõc 4 ‚Äî Positional Encoding (PE)

Self-attention **kh√¥ng bi·∫øt v·ªã tr√≠ tuy·ªát ƒë·ªëi** ‚Üí c·∫ßn th√™m PE:

$$
O' = O + \text{PE}(O)
$$

---

### B∆∞·ªõc 5 ‚Äî Projection

H·ª£p nh·∫•t c√°c head v√† ƒë∆∞a v·ªÅ s·ªë k√™nh ban ƒë·∫ßu:

$$
\text{Out} = \text{Conv}_{1\times1}(O')
$$

---

## 5. V√≠ d·ª• minh h·ªça s·ªë h·ªçc

Gi·∫£ s·ª≠ **feature map** nh·ªè: **3x3** ($H=3, W=3, N=9$), v·ªõi **4 k√™nh** ($\text{dim}=4$).  
D√πng **2 head** (`num_heads=2`), `attn_ratio=0.5` ‚Üí `head_dim = 4 // 2 = 2`, `key_dim = int(2 * 0.5) = 1`, `scale = 1 / \sqrt{1} = 1`.

**Input X** (B=1, C=4, H=3, W=3): C√°c k√™nh tƒÉng d·∫ßn ƒë·ªÉ d·ªÖ theo d√µi.

$$
X[:,0,:,:] = 
\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9
\end{bmatrix},\quad
X[:,1,:,:] = 
\begin{bmatrix}
10 & 11 & 12 \\
13 & 14 & 15 \\
16 & 17 & 18
\end{bmatrix},\quad
X[:,2,:,:] = 
\begin{bmatrix}
19 & 20 & 21 \\
22 & 23 & 24 \\
25 & 26 & 27
\end{bmatrix},\quad
X[:,3,:,:] = 
\begin{bmatrix}
28 & 29 & 30 \\
31 & 32 & 33 \\
34 & 35 & 36
\end{bmatrix}
$$

ƒê·ªÉ minh h·ªça, ch√∫ng ta s·ª≠ d·ª•ng tr·ªçng s·ªë ƒë∆∞·ª£c kh·ªüi t·∫°o ng·∫´u nhi√™n (v·ªõi seed 42 ƒë·ªÉ t√°i t·∫°o), s·ª≠ d·ª•ng ph√¢n ph·ªëi normal (mean=0, std=0.02) cho c√°c conv layers.

---

### 5.1 T·∫°o Q, K, V 

‚ö° **[V√ç D·ª§ Q, K, V ƒë∆∞·ª£c ch·ªçn nh∆∞ n√†o ](../yolo11/EX/attention_module_yolo_3x3_example.md)**

Sau Conv1√ó1 (qkv), tensor g·ªôp QKV c√≥ shape (1,8,3,3). C√°c gi√° tr·ªã t√≠nh ƒë∆∞·ª£c nh∆∞ sau (l√†m tr√≤n ƒë·∫øn 4 ch·ªØ s·ªë):

$$
\text{QKV}[:,0,:,:] = 
\begin{bmatrix}
-0.5008 & -0.4566 & -0.4124 \\
-0.3683 & -0.3241 & -0.2799 \\
-0.2357 & -0.1915 & -0.1473
\end{bmatrix},\quad
\text{QKV}[:,1,:,:] = 
\begin{bmatrix}
-1.1483 & -1.1924 & -1.2365 \\
-1.2806 & -1.3246 & -1.3687 \\
-1.4128 & -1.4569 & -1.5009
\end{bmatrix},
$$

(t∆∞∆°ng t·ª± cho c√°c k√™nh c√≤n l·∫°i: -0.6205 ƒë·∫øn -0.7644 cho k√™nh 2, 0.0084 ƒë·∫øn -0.1986 cho k√™nh 3, 0.0581 ƒë·∫øn 0.2861 cho k√™nh 4, 1.4460 ƒë·∫øn 1.8942 cho k√™nh 5, 1.2643 ƒë·∫øn 1.9876 cho k√™nh 6, 0.3896 ƒë·∫øn 0.4566 cho k√™nh 7).


Sau reshape v√† split:

- **Q** (shape [1,2,1,9]): Flatten theo v·ªã tr√≠.

  - Head 0: [-0.5008, -0.4566, -0.4124, -0.3683, -0.3241, -0.2799, -0.2357, -0.1915, -0.1473]

  - Head 1: [0.0581, 0.0866, 0.1151, 0.1436, 0.1721, 0.2006, 0.2291, 0.2576, 0.2861]

- **K** (shape [1,2,1,9]):

  - Head 0: [-1.1483, -1.1924, -1.2365, -1.2806, -1.3246, -1.3687, -1.4128, -1.4569, -1.5009]

  - Head 1: [1.4460, 1.5020, 1.5580, 1.6140, 1.6701, 1.7261, 1.7821, 1.8382, 1.8942]

- **V** (shape [1,2,2,9]):

  Head 0: 

  - Chi·ªÅu 0: [-0.6205, -0.6384, -0.6564, -0.6744, -0.6924, -0.7104, -0.7284, -0.7464, -0.7644]

  - Chi·ªÅu 1: [0.0084, -0.0175, -0.0434, -0.0693, -0.0951, -0.1210, -0.1469, -0.1728, -0.1986]

  Head 1: 

  - Chi·ªÅu 0: [1.2643, 1.3547, 1.4451, 1.5355, 1.6260, 1.7164, 1.8068, 1.8972, 1.9876]

  - Chi·ªÅu 1: [0.3896, 0.3980, 0.4064, 0.4147, 0.4231, 0.4315, 0.4399, 0.4482, 0.4566]

---

### 5.2 T√≠nh similarity $S = Q^T K \cdot \text{scale}$

#### 5.2.1 T√≠nh similarity $S = Q^T K$

T√≠nh $Q^T K$ cho m·ªói head, shape $[1, 2, 9, 9]$.

**C√¥ng th·ª©c**:

$$ S = Q^T K $$

Cho Head 0, $Q^T$ l√† $[9, 1]$, $K$ l√† $[1, 9]$, n√™n $Q^T K$ l√† ma tr·∫≠n $[9, 9]$. M·ªói ph·∫ßn t·ª≠ $S_{ij} = q_i \cdot k_j$.

T√≠nh to√°n (l√†m tr√≤n ƒë·∫øn 4 ch·ªØ s·ªë):

```
Q_0 = [-0.5008, -0.4566, -0.4124, -0.3683, -0.3241, -0.2799, -0.2357, -0.1915, -0.1473]
K_0 = [-1.1483, -1.1924, -1.2365, -1.2806, -1.3246, -1.3687, -1.4128, -1.4569, -1.5009]
```

H√†ng 0 ($i=0$, $Q_0[0] = -0.5008$):

```
S_0[0, :] = -0.5008 * [-1.1483, -1.1924, -1.2365, -1.2806, -1.3246, -1.3687, -1.4128, -1.4569, -1.5009]
         = [0.5751, 0.5971, 0.6192, 0.6412, 0.6632, 0.6852, 0.7072, 0.7293, 0.7513]
```

Ma tr·∫≠n $S_0$:

$$
S_0 =
\begin{bmatrix}
0.5751 & 0.5971 & 0.6192 & 0.6412 & 0.6632 & 0.6852 & 0.7072 & 0.7293 & 0.7513 \\
0.5244 & 0.5442 & 0.5641 & 0.5840 & 0.6038 & 0.6237 & 0.6436 & 0.6634 & 0.6833 \\
0.4736 & 0.4915 & 0.5094 & 0.5273 & 0.5452 & 0.5631 & 0.5810 & 0.5989 & 0.6168 \\
0.4228 & 0.4388 & 0.4548 & 0.4708 & 0.4868 & 0.5028 & 0.5188 & 0.5348 & 0.5508 \\
0.3720 & 0.3861 & 0.4002 & 0.4142 & 0.4283 & 0.4424 & 0.4565 & 0.4705 & 0.4846 \\
0.3212 & 0.3334 & 0.3455 & 0.3576 & 0.3697 & 0.3818 & 0.3939 & 0.4060 & 0.4181 \\
0.2704 & 0.2807 & 0.2909 & 0.3012 & 0.3114 & 0.3217 & 0.3319 & 0.3422 & 0.3524 \\
0.2196 & 0.2279 & 0.2362 & 0.2445 & 0.2528 & 0.2610 & 0.2693 & 0.2776 & 0.2859 \\
0.1688 & 0.1752 & 0.1816 & 0.1880 & 0.1944 & 0.2008 & 0.2072 & 0.2136 & 0.2200
\end{bmatrix}
$$

Cho Head 1:

```
Q_1 = [0.0581, 0.0866, 0.1151, 0.1436, 0.1721, 0.2006, 0.2291, 0.2576, 0.2861]
K_1 = [1.4460, 1.5020, 1.5580, 1.6140, 1.6701, 1.7261, 1.7821, 1.8382, 1.8942]
```

H√†ng 0:

```
S_1[0, :] = 0.0581 * [1.4460, 1.5020, 1.5580, 1.6140, 1.6701, 1.7261, 1.7821, 1.8382, 1.8942]
         = [0.0840, 0.0873, 0.0905, 0.0938, 0.0970, 0.1003, 0.1035, 0.1068, 0.1100]
```

Ma tr·∫≠n $S_1$:

$$
S_1 =
\begin{bmatrix}
0.0840 & 0.0873 & 0.0905 & 0.0938 & 0.0970 & 0.1003 & 0.1035 & 0.1068 & 0.1100 \\
0.1252 & 0.1300 & 0.1349 & 0.1398 & 0.1447 & 0.1496 & 0.1544 & 0.1593 & 0.1642 \\
0.1664 & 0.1728 & 0.1792 & 0.1857 & 0.1921 & 0.1985 & 0.2049 & 0.2114 & 0.2178 \\
0.2076 & 0.2156 & 0.2236 & 0.2317 & 0.2397 & 0.2477 & 0.2557 & 0.2638 & 0.2718 \\
0.2488 & 0.2584 & 0.2680 & 0.2776 & 0.2872 & 0.2968 & 0.3064 & 0.3160 & 0.3256 \\
0.2900 & 0.3011 & 0.3123 & 0.3235 & 0.3346 & 0.3458 & 0.3569 & 0.3681 & 0.3793 \\
0.3312 & 0.3439 & 0.3566 & 0.3693 & 0.3820 & 0.3947 & 0.4074 & 0.4201 & 0.4328 \\
0.3724 & 0.3867 & 0.4010 & 0.4153 & 0.4296 & 0.4439 & 0.4582 & 0.4725 & 0.4868 \\
0.4136 & 0.4295 & 0.4454 & 0.4613 & 0.4772 & 0.4931 & 0.5090 & 0.5249 & 0.5408
\end{bmatrix}
$$

#### 5.2.2 Scale: Chia cho $\sqrt{d_k}$

V·ªõi $\text{key}_{\text{dim}} = d_k = 1$, n√™n $\sqrt{d_k} = 1$, do ƒë√≥ $\text{scale} = \frac{1}{\sqrt{1}} = 1$. V√¨ v·∫≠y:

$$ S_{\text{scaled}} = S \cdot \text{scale} = S \cdot 1 = S $$

Ma tr·∫≠n $S_0$ v√† $S_1$ kh√¥ng thay ƒë·ªïi.

#### 5.3.4 Softmax theo h√†ng

√Åp d·ª•ng softmax tr√™n t·ª´ng h√†ng c·ªßa $S_0$ v√† $S_1$:

$$ \text{attn}_{ij} = \frac{\exp(S_{ij})}{\sum_j \exp(S_{ij})} $$

**Cho Head 0**:

H√†ng 0 c·ªßa $S_0$:

```
[0.5751, 0.5971, 0.6192, 0.6412, 0.6632, 0.6852, 0.7072, 0.7293, 0.7513]
```

T√≠nh $\exp(S_{ij})$:

```
[exp(0.5751) ‚âà 1.7771, exp(0.5971) ‚âà 1.8167, ..., exp(0.7513) ‚âà 2.1200]
```

T·ªïng: $\sum_j \exp(S_{0j}) \approx 1.7771 + 1.8167 + 1.8570 + 1.8980 + 1.9400 + 1.9830 + 2.0270 + 2.0720 + 2.1200 \approx 17.4908$

Softmax:

```
A_0[0, :] ‚âà [0.1016, 0.1038, 0.1061, 0.1085, 0.1109, 0.1134, 0.1159, 0.1185, 0.1212]
```

Ma tr·∫≠n $A_0$:

$$
A_0 =
\begin{bmatrix}
0.1016 & 0.1038 & 0.1061 & 0.1085 & 0.1109 & 0.1134 & 0.1159 & 0.1185 & 0.1212 \\
0.1024 & 0.1045 & 0.1066 & 0.1088 & 0.1110 & 0.1132 & 0.1155 & 0.1179 & 0.1203 \\
0.1032 & 0.1051 & 0.1070 & 0.1090 & 0.1110 & 0.1130 & 0.1151 & 0.1172 & 0.1194 \\
0.1040 & 0.1057 & 0.1075 & 0.1092 & 0.1110 & 0.1128 & 0.1147 & 0.1166 & 0.1185 \\
0.1049 & 0.1064 & 0.1079 & 0.1095 & 0.1110 & 0.1126 & 0.1143 & 0.1159 & 0.1176 \\
0.1057 & 0.1070 & 0.1083 & 0.1097 & 0.1111 & 0.1124 & 0.1138 & 0.1152 & 0.1167 \\
0.1066 & 0.1077 & 0.1088 & 0.1099 & 0.1111 & 0.1122 & 0.1134 & 0.1146 & 0.1158 \\
0.1074 & 0.1083 & 0.1092 & 0.1102 & 0.1111 & 0.1120 & 0.1130 & 0.1139 & 0.1149 \\
0.1082 & 0.1090 & 0.1097 & 0.1104 & 0.1111 & 0.1118 & 0.1125 & 0.1133 & 0.1140
\end{bmatrix}
$$

**Cho Head 1**:

H√†ng 0 c·ªßa $S_1$:

```
[0.0840, 0.0873, 0.0905, 0.0938, 0.0970, 0.1003, 0.1035, 0.1068, 0.1100]
```

T√≠nh $\exp(S_{1j})$:

```
[exp(0.0840) ‚âà 1.0876, exp(0.0873) ‚âà 1.0911, ..., exp(0.1100) ‚âà 1.1163]
```

T·ªïng: $\sum_j \exp(S_{1j}) \approx 9.9174$

Softmax:

```
A_1[0, :] ‚âà [0.1097, 0.1100, 0.1104, 0.1107, 0.1111, 0.1115, 0.1118, 0.1122, 0.1126]
```

Ma tr·∫≠n $A_1$:

$$
A_1 =
\begin{bmatrix}
0.1097 & 0.1100 & 0.1104 & 0.1107 & 0.1111 & 0.1115 & 0.1118 & 0.1122 & 0.1126 \\
0.1090 & 0.1095 & 0.1100 & 0.1106 & 0.1111 & 0.1116 & 0.1122 & 0.1127 & 0.1133 \\
0.1083 & 0.1090 & 0.1097 & 0.1104 & 0.1111 & 0.1118 & 0.1125 & 0.1133 & 0.1140 \\
0.1076 & 0.1084 & 0.1093 & 0.1102 & 0.1111 & 0.1120 & 0.1129 & 0.1138 & 0.1147 \\
0.1069 & 0.1079 & 0.1090 & 0.1100 & 0.1111 & 0.1122 & 0.1132 & 0.1143 & 0.1154 \\
0.1062 & 0.1074 & 0.1086 & 0.1098 & 0.1111 & 0.1123 & 0.1136 & 0.1149 & 0.1162 \\
0.1055 & 0.1069 & 0.1082 & 0.1096 & 0.1111 & 0.1125 & 0.1139 & 0.1154 & 0.1169 \\
0.1048 & 0.1063 & 0.1079 & 0.1094 & 0.1110 & 0.1126 & 0.1143 & 0.1159 & 0.1176 \\
0.1041 & 0.1058 & 0.1075 & 0.1093 & 0.1110 & 0.1128 & 0.1146 & 0.1165 & 0.1184
\end{bmatrix}
$$

**Ki·ªÉm tra**: T·ªïng m·ªói h√†ng b·∫±ng 1, ƒë√∫ng v·ªõi softmax.

### 5.3 K·∫øt h·ª£p v·ªõi V

$$ O = V @ A^T $$

**Cho Head 0**:

```
V_0 = [[-0.6205, -0.6384, -0.6564, -0.6744, -0.6924, -0.7104, -0.7284, -0.7464, -0.7644],
       [0.0084, -0.0175, -0.0434, -0.0693, -0.0951, -0.1210, -0.1469, -0.1728, -0.1986]]
```

T√≠nh $O_0[:, 0]$:

```
O_0[:, 0] = sum_j (V_0[:, j] * A_0[0, j])
         ‚âà (-0.6205 * 0.1016) + (-0.6384 * 0.1038) + ... + (-0.7644 * 0.1212)
         ‚âà [-0.6951, -0.0989]
```

**Cho Head 1**:

```
V_1 = [[1.2643, 1.3547, 1.4451, 1.5355, 1.6260, 1.7164, 1.8068, 1.8972, 1.9876],
       [0.3896, 0.3980, 0.4064, 0.4147, 0.4231, 0.4315, 0.4399, 0.4482, 0.4566]]
```

K·∫øt qu·∫£ $O$ (reshape v·ªÅ $[1, 4, 3, 3]$):

$$
O[:,0,:,:] =
\begin{bmatrix}
-0.6951 & -0.6948 & -0.6946 \\
-0.6944 & -0.6941 & -0.6939 \\
-0.6937 & -0.6934 & -0.6932
\end{bmatrix}
$$

$$
O[:,1,:,:] =
\begin{bmatrix}
-0.0989 & -0.0986 & -0.0983 \\
-0.0979 & -0.0976 & -0.0973 \\
-0.0969 & -0.0966 & -0.0962
\end{bmatrix}
$$

$$
O[:,2,:,:] =
\begin{bmatrix}
1.6279 & 1.6289 & 1.6298 \\
1.6308 & 1.6318 & 1.6327 \\
1.6337 & 1.6347 & 1.6356
\end{bmatrix}
$$

$$
O[:,3,:,:] =
\begin{bmatrix}
0.4233 & 0.4234 & 0.4235 \\
0.4236 & 0.4237 & 0.4237 \\
0.4238 & 0.4239 & 0.4240
\end{bmatrix}
$$

C√≥ th·ªÉ th·∫•y, sau attention, c√°c gi√° tr·ªã ƒë∆∞·ª£c t·ªïng h·ª£p to√†n c·ª•c, v·ªõi s·ª± ch√∫ √Ω ph√¢n b·ªë nh·∫π nh√†ng tƒÉng theo v·ªã tr√≠ (do gi√° tr·ªã Q v√† K √¢m/d∆∞∆°ng).

---

### 5.4 Positional Encoding (PE)

√Åp d·ª•ng Conv3x3 tr√™n Res ƒë·ªÉ th√™m th√¥ng tin v·ªã tr√≠. K·∫øt qu·∫£ PE(Res):

$$
\text{PE}[:,0,:,:] = 
\begin{bmatrix}
0.0199 & 0.0239 & 0.0210 \\
0.0280 & 0.0447 & 0.0428 \\
-0.0017 & 0.0037 & 0.0213
\end{bmatrix},\quad
\text{PE}[:,1,:,:] = 
\begin{bmatrix}
-0.0058 & -0.0083 & -0.0089 \\
-0.0070 & -0.0095 & -0.0088 \\
-0.0025 & -0.0024 & -0.0033
\end{bmatrix},
$$

(t∆∞∆°ng t·ª± cho k√™nh 2 v√† 3, v·ªõi gi√° tr·ªã t·ª´ -0.0040 ƒë·∫øn 0.1631).

Sau ƒë√≥, O' = Res + PE(Res).

---

### 5.5 Projection

√Åp d·ª•ng Conv1x1 tr√™n O' ƒë·ªÉ ƒë∆∞·ª£c output cu·ªëi (l√†m tr√≤n ƒë·∫øn 4 ch·ªØ s·ªë):

$$
\text{Out}[:,0,:,:] = 
\begin{bmatrix}
0.0283 & 0.0288 & 0.0288 \\
0.0271 & 0.0272 & 0.0275 \\
0.0273 & 0.0274 & 0.0273
\end{bmatrix},\quad
\text{Out}[:,1,:,:] = 
\begin{bmatrix}
-0.0003 & -0.0003 & -0.0002 \\
0.0004 & 0.0007 & 0.0006 \\
0.0006 & 0.0009 & 0.0007
\end{bmatrix},
$$

$$
\text{Out}[:,2,:,:] = 
\begin{bmatrix}
-0.0149 & -0.0161 & -0.0163 \\
-0.0156 & -0.0185 & -0.0186 \\
-0.0152 & -0.0172 & -0.0176
\end{bmatrix},\quad
\text{Out}[:,3,:,:] = 
\begin{bmatrix}
-0.0316 & -0.0331 & -0.0335 \\
-0.0326 & -0.0362 & -0.0362 \\
-0.0326 & -0.0352 & -0.0353
\end{bmatrix}
$$

C√≥ th·ªÉ th·∫•y output ƒë√£ ƒë∆∞·ª£c l√†m gi√†u b·ªüi th√¥ng tin to√†n c·ª•c t·ª´ attention, v·ªõi s·ª± ƒëi·ªÅu ch·ªânh t·ª´ PE v√† projection.

---

## 6. √ù nghƒ©a

- **Q, K**: x√°c ƒë·ªãnh **n√™n ch√∫ √Ω v√†o ƒë√¢u**.
- **V**: ch·ª©a **th√¥ng tin n·ªôi dung**.
- **Softmax(QK^T)**: ph√¢n ph·ªëi x√°c su·∫•t ch√∫ √Ω.
- **PE**: th√™m th√¥ng tin v·ªã tr√≠.
- **Projection**: h·ª£p nh·∫•t c√°c head.

üëâ Attention cho ph√©p m·ªói pixel **nh√¨n to√†n b·ªô ·∫£nh** v√† **t·ª± ch·ªçn th√¥ng tin** ƒë·ªÉ l√†m gi√†u ƒë·∫∑c tr∆∞ng c·ªßa m√¨nh.

---
