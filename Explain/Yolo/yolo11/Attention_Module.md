# Attention Module trong YOLO ‚Äî Gi·∫£i th√≠ch chi ti·∫øt + V√≠ d·ª• minh h·ªça

## 1. Gi·ªõi thi·ªáu

M·ª•c ti√™u c·ªßa **Attention** l√† cho ph√©p m·ªói v·ªã tr√≠ (pixel/patch) trong ·∫£nh **li√™n k·∫øt** v·ªõi t·∫•t c·∫£ c√°c v·ªã tr√≠ kh√°c, ch·ª© kh√¥ng ch·ªâ nh√¨n local nh∆∞ convolution.  
Nh·ªù ƒë√≥, m√¥ h√¨nh h·ªçc ƒë∆∞·ª£c c·∫£:

- Quan h·ªá g·∫ßn (local features).
- Quan h·ªá xa (global context).

---

## 2. Tham s·ªë quan tr·ªçng

- `dim`: t·ªïng s·ªë k√™nh c·ªßa feature ƒë·∫ßu v√†o.
- `num_heads`: chia k√™nh th√†nh nhi·ªÅu *head* ƒë·ªÉ h·ªçc song song.
- `attn_ratio`: ƒë·ªãnh nghƒ©a k√≠ch th∆∞·ªõc c·ªßa Query/Key so v·ªõi Value.

üëâ Hi·ªÉu n√¥m na: **Value** ch·ª©a th√¥ng tin "n·ªôi dung", c√≤n **Query/Key** ch·ªâ ƒë√≥ng vai tr√≤ "so kh·ªõp m·ª©c ƒë·ªô li√™n quan".  
V√¨ th·∫ø th∆∞·ªùng cho Q,K nh·ªè h∆°n V ƒë·ªÉ gi·∫£m chi ph√≠ t√≠nh to√°n.

---

## 3. C√°c thu·ªôc t√≠nh ch√≠nh

- **`head_dim = dim // num_heads`**  
  ‚Üí m·ªói head x·ª≠ l√Ω m·ªôt l√°t c·∫Øt k√™nh ri√™ng bi·ªát.

- **`key_dim = int(head_dim * attn_ratio)`**  
  ‚Üí quy·∫øt ƒë·ªãnh k√≠ch th∆∞·ªõc Q, K.

- **`scale = 1 / \sqrt{key_dim}`**  
  ‚Üí n·∫øu kh√¥ng c√≥ scale, khi t√≠nh $QK^T$, gi√° tr·ªã c√≥ th·ªÉ l·ªõn ‚Üí softmax saturate ‚Üí gradient vanish.

- **`qkv`**: Conv1√ó1 t·∫°o Q, K, V c√πng l√∫c.

- **`proj`**: Conv1√ó1 h·ª£p nh·∫•t k·∫øt qu·∫£ t·ª´ nhi·ªÅu head.

- **`pe`**: Conv3√ó3 (group conv) ƒë·ªÉ b·ªï sung th√¥ng tin v·ªã tr√≠ (positional encoding).

---

## 4. Lu·ªìng x·ª≠ l√Ω (forward)

### B∆∞·ªõc 1 ‚Äî Sinh Q, K, V

- Input: $x$ c√≥ shape $(B, C, H, W)$
- Sau `Conv1√ó1`: t·∫°o tensor g·ªôp `[Q, K, V]`.
- Split:

$$
\begin{aligned}
Q &: (B, \text{num}_{\text{heads}}, \text{key}_{\text{dim}}, N) \\
K &: (B, \text{num}_{\text{heads}}, \text{key}_{\text{dim}}, N) \\
V &: (B, \text{num}_{\text{heads}}, \text{head}_{\text{dim}}, N)
\end{aligned}
$$



v·ªõi $N = H \times W$ (t·ªïng s·ªë pixel).

---

### B∆∞·ªõc 2 ‚Äî Attention Score

T√≠nh ƒë·ªô t∆∞∆°ng t·ª± (similarity) gi·ªØa Q v√† K:

$$
S = Q^T K \cdot \text{scale}
$$

- $S$ c√≥ shape $(B, \text{num\_heads}, N, N)$.
- M·ªói ph·∫ßn t·ª≠ $s_{ij}$ = m·ª©c ƒë·ªô li√™n quan c·ªßa v·ªã tr√≠ $i$ ƒë·∫øn v·ªã tr√≠ $j$.

Chu·∫©n h√≥a softmax theo t·ª´ng h√†ng:

$$
\text{attn}_{ij} = \frac{\exp(s_{ij})}{\sum_{j} \exp(s_{ij})}
$$

---

### B∆∞·ªõc 3 ‚Äî K·∫øt h·ª£p v·ªõi V

T·∫°o output b·∫±ng c√°ch nh√¢n attention v·ªõi V:

$$
O = \text{attn} \cdot V^T
$$

---

### B∆∞·ªõc 4 ‚Äî Positional Encoding (PE)

Self-attention **kh√¥ng bi·∫øt v·ªã tr√≠ tuy·ªát ƒë·ªëi** ‚Üí c·∫ßn th√™m PE:

$$
O' = O + \text{PE}(O)
$$

---

### B∆∞·ªõc 5 ‚Äî Projection

H·ª£p nh·∫•t c√°c head v√† ƒë∆∞a v·ªÅ s·ªë k√™nh ban ƒë·∫ßu:

$$
\text{Out} = \text{Conv}_{1\times1}(O')
$$

---

## 5. V√≠ d·ª• minh h·ªça s·ªë h·ªçc

Gi·∫£ s·ª≠ **feature map** nh·ªè: **3x3** ($H=3, W=3, N=9$), v·ªõi **4 k√™nh** ($\text{dim}=4$).  
D√πng **2 head** (`num_heads=2`), `attn_ratio=0.5` ‚Üí `head_dim = 4 // 2 = 2`, `key_dim = int(2 * 0.5) = 1`, `scale = 1 / \sqrt{1} = 1`.

**Input X** (B=1, C=4, H=3, W=3): C√°c k√™nh tƒÉng d·∫ßn ƒë·ªÉ d·ªÖ theo d√µi.

$$
X[:,0,:,:] = 
\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9
\end{bmatrix},\quad
X[:,1,:,:] = 
\begin{bmatrix}
10 & 11 & 12 \\
13 & 14 & 15 \\
16 & 17 & 18
\end{bmatrix},\quad
X[:,2,:,:] = 
\begin{bmatrix}
19 & 20 & 21 \\
22 & 23 & 24 \\
25 & 26 & 27
\end{bmatrix},\quad
X[:,3,:,:] = 
\begin{bmatrix}
28 & 29 & 30 \\
31 & 32 & 33 \\
34 & 35 & 36
\end{bmatrix}
$$

ƒê·ªÉ minh h·ªça, ch√∫ng ta s·ª≠ d·ª•ng tr·ªçng s·ªë ƒë∆∞·ª£c kh·ªüi t·∫°o ng·∫´u nhi√™n (v·ªõi seed 42 ƒë·ªÉ t√°i t·∫°o), s·ª≠ d·ª•ng ph√¢n ph·ªëi normal (mean=0, std=0.02) cho c√°c conv layers.

---

### 5.1 T·∫°o Q, K, V 

‚ö° **[V√ç D·ª§ Q, K, V ƒë∆∞·ª£c ch·ªçn nh∆∞ n√†o ](../yolo11/EX/attention_module_yolo_3x3_example.md)**

Sau Conv1√ó1 (qkv), tensor g·ªôp QKV c√≥ shape (1,8,3,3). C√°c gi√° tr·ªã t√≠nh ƒë∆∞·ª£c nh∆∞ sau (l√†m tr√≤n ƒë·∫øn 4 ch·ªØ s·ªë):

$$
\text{QKV}[:,0,:,:] = 
\begin{bmatrix}
-0.5008 & -0.4566 & -0.4124 \\
-0.3683 & -0.3241 & -0.2799 \\
-0.2357 & -0.1915 & -0.1473
\end{bmatrix},\quad
\text{QKV}[:,1,:,:] = 
\begin{bmatrix}
-1.1483 & -1.1924 & -1.2365 \\
-1.2806 & -1.3246 & -1.3687 \\
-1.4128 & -1.4569 & -1.5009
\end{bmatrix},
$$

(t∆∞∆°ng t·ª± cho c√°c k√™nh c√≤n l·∫°i: -0.6205 ƒë·∫øn -0.7644 cho k√™nh 2, 0.0084 ƒë·∫øn -0.1986 cho k√™nh 3, 0.0581 ƒë·∫øn 0.2861 cho k√™nh 4, 1.4460 ƒë·∫øn 1.8942 cho k√™nh 5, 1.2643 ƒë·∫øn 1.9876 cho k√™nh 6, 0.3896 ƒë·∫øn 0.4566 cho k√™nh 7).


Sau reshape v√† split:

- **Q** (shape [1,2,1,9]): Flatten theo v·ªã tr√≠.

  - Head 0: [-0.5008, -0.4566, -0.4124, -0.3683, -0.3241, -0.2799, -0.2357, -0.1915, -0.1473]

  - Head 1: [0.0581, 0.0866, 0.1151, 0.1436, 0.1721, 0.2006, 0.2291, 0.2576, 0.2861]

- **K** (shape [1,2,1,9]):

  - Head 0: [-1.1483, -1.1924, -1.2365, -1.2806, -1.3246, -1.3687, -1.4128, -1.4569, -1.5009]

  - Head 1: [1.4460, 1.5020, 1.5580, 1.6140, 1.6701, 1.7261, 1.7821, 1.8382, 1.8942]

- **V** (shape [1,2,2,9]):

  Head 0: 

  - Chi·ªÅu 0: [-0.6205, -0.6384, -0.6564, -0.6744, -0.6924, -0.7104, -0.7284, -0.7464, -0.7644]

  - Chi·ªÅu 1: [0.0084, -0.0175, -0.0434, -0.0693, -0.0951, -0.1210, -0.1469, -0.1728, -0.1986]

  Head 1: 

  - Chi·ªÅu 0: [1.2643, 1.3547, 1.4451, 1.5355, 1.6260, 1.7164, 1.8068, 1.8972, 1.9876]

  - Chi·ªÅu 1: [0.3896, 0.3980, 0.4064, 0.4147, 0.4231, 0.4315, 0.4399, 0.4482, 0.4566]

---

### 5.2 T√≠nh similarity $S = Q^T K \cdot \text{scale}$

V√¨ key_dim=1, $Q^T K$ l√† ma tr·∫≠n [9,9] cho m·ªói head, v·ªõi $S_{ij} = q_i \cdot k_j \cdot 1$.

Sau ƒë√≥ √°p d·ª•ng softmax ƒë·ªÉ ƒë∆∞·ª£c attn (l√†m tr√≤n ƒë·∫øn 4 ch·ªØ s·ªë). D∆∞·ªõi ƒë√¢y l√† attn cho Head 0 (ma tr·∫≠n 9x9):

$$
A_0 \approx
\begin{bmatrix}
0.1016 & 0.1038 & 0.1061 & 0.1085 & 0.1109 & 0.1134 & 0.1159 & 0.1185 & 0.1212 \\
0.1024 & 0.1045 & 0.1066 & 0.1088 & 0.1110 & 0.1132 & 0.1155 & 0.1179 & 0.1203 \\
0.1032 & 0.1051 & 0.1070 & 0.1090 & 0.1110 & 0.1130 & 0.1151 & 0.1172 & 0.1194 \\
0.1040 & 0.1057 & 0.1075 & 0.1092 & 0.1110 & 0.1128 & 0.1147 & 0.1166 & 0.1185 \\
0.1049 & 0.1064 & 0.1079 & 0.1095 & 0.1110 & 0.1126 & 0.1143 & 0.1159 & 0.1176 \\
0.1057 & 0.1070 & 0.1083 & 0.1097 & 0.1111 & 0.1124 & 0.1138 & 0.1152 & 0.1167 \\
0.1066 & 0.1077 & 0.1088 & 0.1099 & 0.1111 & 0.1122 & 0.1134 & 0.1146 & 0.1158 \\
0.1074 & 0.1083 & 0.1092 & 0.1102 & 0.1111 & 0.1120 & 0.1130 & 0.1139 & 0.1149 \\
0.1082 & 0.1090 & 0.1097 & 0.1104 & 0.1111 & 0.1118 & 0.1125 & 0.1133 & 0.1140
\end{bmatrix}
$$

Attn cho Head 1 (t∆∞∆°ng t·ª±, nh∆∞ng gi√° tr·ªã kh√°c):

$$
A_1 \approx
\begin{bmatrix}
0.1097 & 0.1100 & 0.1104 & 0.1107 & 0.1111 & 0.1115 & 0.1118 & 0.1122 & 0.1126 \\
0.1090 & 0.1095 & 0.1100 & 0.1106 & 0.1111 & 0.1116 & 0.1122 & 0.1127 & 0.1133 \\
0.1083 & 0.1090 & 0.1097 & 0.1104 & 0.1111 & 0.1118 & 0.1125 & 0.1133 & 0.1140 \\
0.1076 & 0.1084 & 0.1093 & 0.1102 & 0.1111 & 0.1120 & 0.1129 & 0.1138 & 0.1147 \\
0.1069 & 0.1079 & 0.1090 & 0.1100 & 0.1111 & 0.1122 & 0.1132 & 0.1143 & 0.1154 \\
0.1062 & 0.1074 & 0.1086 & 0.1098 & 0.1111 & 0.1123 & 0.1136 & 0.1149 & 0.1162 \\
0.1055 & 0.1069 & 0.1082 & 0.1096 & 0.1111 & 0.1125 & 0.1139 & 0.1154 & 0.1169 \\
0.1048 & 0.1063 & 0.1079 & 0.1094 & 0.1110 & 0.1126 & 0.1143 & 0.1159 & 0.1176 \\
0.1041 & 0.1058 & 0.1075 & 0.1093 & 0.1110 & 0.1128 & 0.1146 & 0.1165 & 0.1184
\end{bmatrix}
$$

---

### 5.3 K·∫øt h·ª£p v·ªõi V (Res sau attention)

$O = V @ A^T$ cho m·ªói head, sau ƒë√≥ gh√©p v√† reshape v·ªÅ [1,4,3,3].

K·∫øt qu·∫£ Res (l√†m tr√≤n ƒë·∫øn 4 ch·ªØ s·ªë):

$$
\text{Res}[:,0,:,:] = 
\begin{bmatrix}
-0.6951 & -0.6948 & -0.6946 \\
-0.6944 & -0.6941 & -0.6939 \\
-0.6937 & -0.6934 & -0.6932
\end{bmatrix},\quad
\text{Res}[:,1,:,:] = 
\begin{bmatrix}
-0.0989 & -0.0986 & -0.0983 \\
-0.0979 & -0.0976 & -0.0973 \\
-0.0969 & -0.0966 & -0.0962
\end{bmatrix},
$$

$$
\text{Res}[:,2,:,:] = 
\begin{bmatrix}
1.6279 & 1.6289 & 1.6298 \\
1.6308 & 1.6318 & 1.6327 \\
1.6337 & 1.6347 & 1.6356
\end{bmatrix},\quad
\text{Res}[:,3,:,:] = 
\begin{bmatrix}
0.4233 & 0.4234 & 0.4235 \\
0.4236 & 0.4237 & 0.4237 \\
0.4238 & 0.4239 & 0.4240
\end{bmatrix}
$$

C√≥ th·ªÉ th·∫•y, sau attention, c√°c gi√° tr·ªã ƒë∆∞·ª£c t·ªïng h·ª£p to√†n c·ª•c, v·ªõi s·ª± ch√∫ √Ω ph√¢n b·ªë nh·∫π nh√†ng tƒÉng theo v·ªã tr√≠ (do gi√° tr·ªã Q v√† K √¢m/d∆∞∆°ng).

---

### 5.4 Positional Encoding (PE)

√Åp d·ª•ng Conv3x3 tr√™n Res ƒë·ªÉ th√™m th√¥ng tin v·ªã tr√≠. K·∫øt qu·∫£ PE(Res):

$$
\text{PE}[:,0,:,:] = 
\begin{bmatrix}
0.0199 & 0.0239 & 0.0210 \\
0.0280 & 0.0447 & 0.0428 \\
-0.0017 & 0.0037 & 0.0213
\end{bmatrix},\quad
\text{PE}[:,1,:,:] = 
\begin{bmatrix}
-0.0058 & -0.0083 & -0.0089 \\
-0.0070 & -0.0095 & -0.0088 \\
-0.0025 & -0.0024 & -0.0033
\end{bmatrix},
$$

(t∆∞∆°ng t·ª± cho k√™nh 2 v√† 3, v·ªõi gi√° tr·ªã t·ª´ -0.0040 ƒë·∫øn 0.1631).

Sau ƒë√≥, O' = Res + PE(Res).

---

### 5.5 Projection

√Åp d·ª•ng Conv1x1 tr√™n O' ƒë·ªÉ ƒë∆∞·ª£c output cu·ªëi (l√†m tr√≤n ƒë·∫øn 4 ch·ªØ s·ªë):

$$
\text{Out}[:,0,:,:] = 
\begin{bmatrix}
0.0283 & 0.0288 & 0.0288 \\
0.0271 & 0.0272 & 0.0275 \\
0.0273 & 0.0274 & 0.0273
\end{bmatrix},\quad
\text{Out}[:,1,:,:] = 
\begin{bmatrix}
-0.0003 & -0.0003 & -0.0002 \\
0.0004 & 0.0007 & 0.0006 \\
0.0006 & 0.0009 & 0.0007
\end{bmatrix},
$$

$$
\text{Out}[:,2,:,:] = 
\begin{bmatrix}
-0.0149 & -0.0161 & -0.0163 \\
-0.0156 & -0.0185 & -0.0186 \\
-0.0152 & -0.0172 & -0.0176
\end{bmatrix},\quad
\text{Out}[:,3,:,:] = 
\begin{bmatrix}
-0.0316 & -0.0331 & -0.0335 \\
-0.0326 & -0.0362 & -0.0362 \\
-0.0326 & -0.0352 & -0.0353
\end{bmatrix}
$$

C√≥ th·ªÉ th·∫•y output ƒë√£ ƒë∆∞·ª£c l√†m gi√†u b·ªüi th√¥ng tin to√†n c·ª•c t·ª´ attention, v·ªõi s·ª± ƒëi·ªÅu ch·ªânh t·ª´ PE v√† projection.

---

## 6. √ù nghƒ©a

- **Q, K**: x√°c ƒë·ªãnh **n√™n ch√∫ √Ω v√†o ƒë√¢u**.
- **V**: ch·ª©a **th√¥ng tin n·ªôi dung**.
- **Softmax(QK^T)**: ph√¢n ph·ªëi x√°c su·∫•t ch√∫ √Ω.
- **PE**: th√™m th√¥ng tin v·ªã tr√≠.
- **Projection**: h·ª£p nh·∫•t c√°c head.

üëâ Attention cho ph√©p m·ªói pixel **nh√¨n to√†n b·ªô ·∫£nh** v√† **t·ª± ch·ªçn th√¥ng tin** ƒë·ªÉ l√†m gi√†u ƒë·∫∑c tr∆∞ng c·ªßa m√¨nh.