

# üîπ V√≠ d·ª• minh h·ªça PSABlock ‚Äî T√≠nh to√°n chi ti·∫øt (theo t·ª´ng b∆∞·ªõc)


---

## 0) Input sau Positional Encoding (X')

D·ªØ li·ªáu ƒë·∫ßu v√†o `x` sau khi √°p d·ª•ng Positional Encoding (PE), ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng tensor v·ªõi 4 k√™nh (`c0'`, `c1'`, `c2'`, `c3'`) cho 4 pixel (`p0`, `p1`, `p2`, `p3`):

| pixel | c0' | c1' | c2' | c3' |
|-------|-----|-----|-----|-----|
| p0    | 1.1 | 0.0 | 1.1 | 2.0 |
| p1    | 2.2 | 1.0 | 0.2 | 2.0 |
| p2    | 3.3 | 0.0 | 1.3 | 2.0 |
| p3    | 4.4 | 1.0 | 0.4 | 2.0 |

> ƒê√¢y l√† `x` (sau PE) ‚Äî ƒë·∫ßu v√†o cho `PSABlock`. M·ªói pixel c√≥ 4 chi·ªÅu (4 k√™nh), v√† ta s·∫Ω gi·∫£ s·ª≠ ƒë·∫ßu ra c·ªßa PSABlock c≈©ng gi·ªØ nguy√™n 4 k√™nh (do `c` l√† s·ªë k√™nh v√†o/ra, m·∫∑c ƒë·ªãnh b·∫±ng 4 trong v√≠ d·ª• n√†y).

---

## 1) K·∫øt qu·∫£ Attention (ƒë√£ c√≥)

K·∫øt qu·∫£ t·ª´ `self.attn(x)` ƒë∆∞·ª£c cung c·∫•p s·∫µn, v·ªõi m·ªói pixel ƒë∆∞·ª£c chi·∫øu v·ªÅ 2 chi·ªÅu (do `PSABlock` s·ª≠ d·ª•ng attention v·ªõi k√≠ch th∆∞·ªõc ƒë·∫ßu ra gi·∫£m xu·ªëng, ph√π h·ª£p v·ªõi c√°ch thi·∫øt k·∫ø trong YOLOv11):

| pixel | out_attn[0] | out_attn[1] |
|-------|-------------|-------------|
| p0    | 4.746       | 2.952       |
| p1    | 4.742       | 2.954       |
| p2    | 4.796       | 2.982       |
| p3    | 4.728       | 2.962       |

> `out_attn` l√† ƒë·∫ßu ra c·ªßa `self.attn(x)`, s·ª≠ d·ª•ng multi-head attention v·ªõi `num_heads=4` (theo tham s·ªë m·∫∑c ƒë·ªãnh) v√† `attn_ratio=0.5`. K·∫øt qu·∫£ n√†y ƒë√£ ƒë∆∞·ª£c t√≠nh tr∆∞·ªõc, ph·∫£n √°nh s·ª± t·∫≠p trung v√†o c√°c m·ªëi quan h·ªá kh√¥ng gian gi·ªØa c√°c pixel.

---

## 2) Projection: Chi·∫øu `x` (4-dim) v·ªÅ 2-dim ƒë·ªÉ chu·∫©n b·ªã cho residual

Gi·∫£i th√≠ch: ƒê·ªÉ th·ª±c hi·ªán residual connection, ta c·∫ßn √°nh x·∫° `x` (4 chi·ªÅu) v·ªÅ c√πng kh√¥ng gian 2 chi·ªÅu c·ªßa `out_attn`. ƒêi·ªÅu n√†y ƒë∆∞·ª£c th·ª±c hi·ªán b·∫±ng c√°ch s·ª≠ d·ª•ng m·ªôt ma tr·∫≠n chi·∫øu `W` (4√ó2), t∆∞∆°ng t·ª± ma tr·∫≠n ƒë√£ d√πng trong `self.attn` ƒë·ªÉ t·∫°o Query/Key/Value. M·ª•c ƒë√≠ch l√† t·∫°o m·ªôt bi·ªÉu di·ªÖn trung gian ƒë·ªÉ c·ªông v·ªõi `out_attn` v√† `ffn_out`.

**C√¥ng th·ª©c:**

$$
\text{proj}(x) = x \cdot W
$$

Gi·∫£ s·ª≠ ma tr·∫≠n `W` (4√ó2) ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a nh∆∞ sau (l·∫•y v√≠ d·ª• ƒë·ªÉ t√≠nh tay, t∆∞∆°ng th√≠ch v·ªõi ƒë·∫ßu ra 2 chi·ªÅu c·ªßa attention):

```
W = [[1, 0],
     [0, 1],
     [1, 0],
     [0, 1]]
```

√Åp d·ª•ng cho t·ª´ng pixel:

- **p0**: `x(p0) = [1.1, 0.0, 1.1, 2.0]`

$$
\text{proj}(x(p0)) = [1.1 \cdot 1 + 0.0 \cdot 0 + 1.1 \cdot 1 + 2.0 \cdot 0, 1.1 \cdot 0 + 0.0 \cdot 1 + 1.1 \cdot 0 + 2.0 \cdot 1] = [2.2, 2.0]
$$

- **p1**: `x(p1) = [2.2, 1.0, 0.2, 2.0]`

$$
\text{proj}(x(p1)) = [2.2 \cdot 1 + 1.0 \cdot 0 + 0.2 \cdot 1 + 2.0 \cdot 0, 2.2 \cdot 0 + 1.0 \cdot 1 + 0.2 \cdot 0 + 2.0 \cdot 1] = [2.4, 3.0]
$$

- **p2**: `x(p2) = [3.3, 0.0, 1.3, 2.0]`

$$
\text{proj}(x(p2)) = [3.3 \cdot 1 + 0.0 \cdot 0 + 1.3 \cdot 1 + 2.0 \cdot 0, 3.3 \cdot 0 + 0.0 \cdot 1 + 1.3 \cdot 0 + 2.0 \cdot 1] = [4.6, 2.0]
$$

- **p3**: `x(p3) = [4.4, 1.0, 0.4, 2.0]`

$$
\text{proj}(x(p3)) = [4.4 \cdot 1 + 1.0 \cdot 0 + 0.4 \cdot 1 + 2.0 \cdot 0, 4.4 \cdot 0 + 1.0 \cdot 1 + 0.4 \cdot 0 + 2.0 \cdot 1] = [4.8, 3.0]
$$

**B·∫£ng `proj(x)` (4‚Üí2):**

| pixel | proj(x)[0] | proj(x)[1] |
|-------|------------|------------|
| p0    | 2.200      | 2.000      |
| p1    | 2.400      | 3.000      |
| p2    | 4.600      | 2.000      |
| p3    | 4.800      | 3.000      |

> L∆∞u √Ω: Ma tr·∫≠n `W` l√† gi·∫£ ƒë·ªãnh ƒë·ªÉ minh h·ªça. Trong th·ª±c t·∫ø, n√≥ ƒë∆∞·ª£c h·ªçc t·ª´ d·ªØ li·ªáu v√† c√≥ th·ªÉ kh√°c, nh∆∞ng k·∫øt qu·∫£ 2 chi·ªÅu ph√π h·ª£p v·ªõi `out_attn`.

---

## 3) FFN (Feed-Forward Network): Conv 2 ‚Üí 4 ‚Üí 2

Gi·∫£i th√≠ch: `self.ffn` bao g·ªìm hai l·ªõp Conv 1x1:
- L·ªõp ƒë·∫ßu (`Conv(c, c * 2, 1)`): M·ªü r·ªông t·ª´ 2 chi·ªÅu l√™n 4 chi·ªÅu.
- L·ªõp sau (`Conv(c * 2, c, 1, act=False)`): N√©n l·∫°i t·ª´ 4 chi·ªÅu v·ªÅ 2 chi·ªÅu, kh√¥ng d√πng activation cu·ªëi ƒë·ªÉ gi·ªØ t√≠nh ch·∫•t tuy·∫øn t√≠nh khi c·ªông residual.

**Tr·ªçng s·ªë minh h·ªça** (gi·∫£ ƒë·ªãnh ƒë·ªÉ t√≠nh tay):
- `W1` (2√ó4): M·ªü r·ªông t·ª´ 2‚Üí4
  ```
  W1 = [[2, 0],
        [0, 2],
        [1, 0],
        [0, 1]]
  ```
- `W2` (4√ó2): N√©n t·ª´ 4‚Üí2
  ```
  W2 = [[1, 0, 1, 0],
        [0, 1, 0, 1]]
  ```

**C√¥ng th·ª©c:**

$$
h = W_1 \cdot \text{out\_attn}, \quad \text{ffn\_out} = W_2 \cdot h
$$

### 3.1 T√≠nh chi ti·∫øt cho p0
- `out_attn(p0) = [4.746, 2.952]`

T√≠nh `h = W1 ¬∑ out_attn`:

$$
h[0] = 2 \cdot 4.746 + 0 \cdot 2.952 = 9.492
$$

$$
h[1] = 0 \cdot 4.746 + 2 \cdot 2.952 = 5.904
$$

$$
h[2] = 1 \cdot 4.746 + 0 \cdot 2.952 = 4.746
$$

$$
h[3] = 0 \cdot 4.746 + 1 \cdot 2.952 = 2.952
$$

‚áí `h = [9.492, 5.904, 4.746, 2.952]`

T√≠nh `ffn_out = W2 ¬∑ h`:

$$
ffn\_out[0] = 1 \cdot 9.492 + 0 \cdot 5.904 + 1 \cdot 4.746 + 0 \cdot 2.952 = 9.492 + 4.746 = 14.238
$$

$$
ffn\_out[1] = 0 \cdot 9.492 + 1 \cdot 5.904 + 0 \cdot 4.746 + 1 \cdot 2.952 = 5.904 + 2.952 = 8.856
$$

‚áí **FFN(p0) = [14.238, 8.856]**

### 3.2 T√≠nh cho p1, p2, p3
- **p1**: `out_attn = [4.742, 2.954]`
  - `h = [9.484, 5.908, 4.742, 2.954]`
  - `ffn_out = [9.484 + 4.742, 5.908 + 2.954] = [14.226, 8.862]`

- **p2**: `out_attn = [4.796, 2.982]`
  - `h = [9.592, 5.964, 4.796, 2.982]`
  - `ffn_out = [9.592 + 4.796, 5.964 + 2.982] = [14.388, 8.946]`

- **p3**: `out_attn = [4.728, 2.962]`
  - `h = [9.456, 5.924, 4.728, 2.962]`
  - `ffn_out = [9.456 + 4.728, 5.924 + 2.962] = [14.184, 8.886]`

**B·∫£ng FFN outputs:**

| pixel | ffn_out[0] | ffn_out[1] |
|-------|------------|------------|
| p0    | 14.238     | 8.856      |
| p1    | 14.226     | 8.862      |
| p2    | 14.388     | 8.946      |
| p3    | 14.184     | 8.886      |

---

## 4) Residual (Shortcut) ‚Äî C·ªông l·∫°i ƒë·ªÉ ra `out_final`

Gi·∫£i th√≠ch: V√¨ `self.add = True`, ta th·ª±c hi·ªán residual connection b·∫±ng c√°ch c·ªông `proj(x)`, `out_attn`, v√† `ffn_out`. M·ªói th√†nh ph·∫ßn ƒë·ªÅu c√≥ 2 chi·ªÅu, ph√π h·ª£p ƒë·ªÉ t·ªïng h·ª£p.

**C√¥ng th·ª©c:**

$$
out\_final = \text{proj}(x) + out\_attn + ffn\_out
$$

### T√≠nh t·ª´ng pixel:
- **p0**:
  - `proj(x) = [2.200, 2.000]`
  - `out_attn = [4.746, 2.952]`
  - `ffn_out = [14.238, 8.856]`

$$
out\_final(p0) = [2.200 + 4.746 + 14.238, 2.000 + 2.952 + 8.856] = [21.184, 13.808]
$$

- **p1**:
  - `proj(x) = [2.400, 3.000]`
  - `out_attn = [4.742, 2.954]`
  - `ffn_out = [14.226, 8.862]`

$$
out\_final(p1) = [2.400 + 4.742 + 14.226, 3.000 + 2.954 + 8.862] = [21.368, 14.816]
$$

- **p2**:
  - `proj(x) = [4.600, 2.000]`
  - `out_attn = [4.796, 2.982]`
  - `ffn_out = [14.388, 8.946]`

$$
out\_final(p2) = [4.600 + 4.796 + 14.388, 2.000 + 2.982 + 8.946] = [23.784, 13.928]
$$

- **p3**:
  - `proj(x) = [4.800, 3.000]`
  - `out_attn = [4.728, 2.962]`
  - `ffn_out = [14.184, 8.886]`

$$
out\_final(p3) = [4.800 + 4.728 + 14.184, 3.000 + 2.962 + 8.886] = [23.712, 14.848]
$$

---

## 5) B·∫£ng k·∫øt qu·∫£ cu·ªëi c√πng (PSABlock output)

| pixel | out_final[0] | out_final[1] |
|-------|--------------|--------------|
| p0    | 21.184       | 13.808       |
| p1    | 21.368       | 14.816       |
| p2    | 23.784       | 13.928       |
| p3    | 23.712       | 14.848       |

---

## Ghi ch√∫ ng·∫Øn
- **Projection (`proj(x)`)**: S·ª≠ d·ª•ng ma tr·∫≠n `W` (4√ó2) ƒë·ªÉ √°nh x·∫° t·ª´ 4 chi·ªÅu v·ªÅ 2 chi·ªÅu, ph√π h·ª£p v·ªõi `out_attn`. Ma tr·∫≠n n√†y l√† gi·∫£ ƒë·ªãnh minh h·ªça, trong th·ª±c t·∫ø ƒë∆∞·ª£c h·ªçc t·ª´ d·ªØ li·ªáu.
- **FFN**: Tr·ªçng s·ªë `W1` v√† `W2` l√† v√≠ d·ª• ƒë·ªÉ t√≠nh tay; trong code th·ª±c t·∫ø, ch√∫ng ƒë∆∞·ª£c hu·∫•n luy·ªán v√† c√≥ th·ªÉ ph·ª©c t·∫°p h∆°n.
- **Residual**: V·ªõi `self.add = True`, c·∫£ `out_attn` v√† `ffn_out` ƒë·ªÅu ƒë∆∞·ª£c c·ªông v·ªõi `proj(x)`, t·∫°o ra ƒë·∫ßu ra 2 chi·ªÅu cho m·ªói pixel.
- **Tr√¨nh t·ª± t·ªïng qu√°t**: **Input (4-dim after PE)** ‚Üí **Projection (4‚Üí2)** ‚Üí **Attention (‚Üíout_attn)** ‚Üí **FFN (2‚Üí4‚Üí2)** ‚Üí **Residual sum** ‚Üí **Output (2-dim per token)**.

---
---

## 1. Projection l√† g√¨?

* Trong to√°n tuy·∫øn t√≠nh, **projection** (chi·∫øu) l√† ph√©p nh√¢n ma tr·∫≠n ƒë·ªÉ ƒë∆∞a vector t·ª´ m·ªôt kh√¥ng gian n√†y sang kh√¥ng gian kh√°c.
* Trong deep learning (Attention, FFN, ‚Ä¶), ‚Äúprojection‚Äù th∆∞·ªùng d√πng ƒë·ªÉ **bi·∫øn ƒë·ªïi s·ªë chi·ªÅu** c·ªßa vector ƒë·∫∑c tr∆∞ng.

V√≠ d·ª•:

* Input c√≥ `dim=4` (vector 4 chi·ªÅu).
* Ta nh√¢n v·ªõi ma tr·∫≠n `W (4√ó2)` ‚Üí ra vector 2 chi·ªÅu.
* ƒê√¢y g·ªçi l√† **chi·∫øu t·ª´ 4D ‚Üí 2D**.

---

## 2. Projection trong Attention

* Khi t·∫°o Q, K, V, ta c√≥:

$$
Q = X W_Q, \quad K = X W_K, \quad V = X W_V
$$

·ªû ƒë√¢y:

* `X`: ƒë·∫ßu v√†o (dim=4).
* `W_Q, W_K, W_V`: c√°c ma tr·∫≠n chi·∫øu (projection matrix).
* K·∫øt qu·∫£:

  * Q v√† K th∆∞·ªùng c√≥ `key_dim` nh·ªè h∆°n (v√≠ d·ª• 2).
  * V th∆∞·ªùng gi·ªØ l·∫°i dim g·ªëc.

üëâ Nh·ªù projection m√† ta:

* Gi·∫£m chi·ªÅu (ƒë·ª° t·ªën t√≠nh to√°n trong dot-product).
* T·∫°o kh√¥ng gian bi·ªÉu di·ªÖn ri√™ng cho Q, K, V.

---

## 3. Projection trong Feed-Forward (FFN)

FFN d√πng hai ph√©p chi·∫øu:

1. **Expand (projection up):** t·ª´ `dim` nh·ªè ‚Üí `dim` l·ªõn (v√≠ d·ª• 2 ‚Üí 4).

   * M·ªü r·ªông kh√¥ng gian ƒë·ªÉ m·∫°ng h·ªçc bi·ªÉu di·ªÖn phi tuy·∫øn ph·ª©c t·∫°p.
2. **Compress (projection down):** t·ª´ `dim` l·ªõn ‚Üí `dim` g·ªëc (v√≠ d·ª• 4 ‚Üí 2).

   * Tr·∫£ output v·ªÅ c√πng k√≠ch th∆∞·ªõc ƒë·ªÉ c·ªông residual.

üëâ ƒê√¢y ch√≠nh l√† l√Ω do FFN m·∫°nh h∆°n 1 linear ƒë∆°n: n√≥ ‚Äúƒëi m·ªôt v√≤ng‚Äù qua kh√¥ng gian ·∫©n l·ªõn h∆°n.

---

## 4. C√¥ng th·ª©c t·ªïng qu√°t Projection

V·ªõi vector $x \in \mathbb{R}^d$ v√† ma tr·∫≠n $W \in \mathbb{R}^{d \times d'}$:

$$
\text{proj}(x) = xW \quad \in \mathbb{R}^{d'}
$$

* N·∫øu $d' < d$: **gi·∫£m chi·ªÅu**.
* N·∫øu $d' > d$: **m·ªü r·ªông chi·ªÅu**.

---

‚úÖ T√≥m l·∫°i:

* **Projection = Linear mapping (chi·∫øu tuy·∫øn t√≠nh)**.
* M·ª•c ti√™u: thay ƒë·ªïi s·ªë chi·ªÅu ƒë·ªÉ t√≠nh to√°n Attention ho·∫∑c t·∫°o kh√¥ng gian ·∫©n trong FFN.

---
